{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bee495-75b7-4d1d-8f8b-53e142e7deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "from torchvision.transforms import Compose, Normalize, Pad\n",
    "\n",
    "from data_generation.square_sequences import generate_sequences\n",
    "from data_generation.image_classification import generate_dataset\n",
    "from helpers import index_splitter, make_balanced_sampler\n",
    "from stepbystep.v4 import StepByStep\n",
    "# These are the classes we built in Chapter 9\n",
    "from seq2seq import PositionalEncoding, subsequent_mask, EncoderDecoderSelfAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc49b51e-f790-4074-be91-f7b29feceed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = int(d_model / n_heads)\n",
    "        self.linear_query = nn.Linear(d_model, d_model)\n",
    "        self.linear_key = nn.Linear(d_model, d_model)\n",
    "        self.linear_value = nn.Linear(d_model, d_model)\n",
    "        self.linear_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.alphas = None\n",
    "\n",
    "    def make_chunks(self, x):\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        # N, L, D -> N, L, n_heads * d_k\n",
    "        x = x.view(batch_size, seq_len, self.n_heads, self.d_k)\n",
    "        # N, n_heads, L, d_k\n",
    "        x = x.transpose(1,2)\n",
    "        return x\n",
    "\n",
    "    def init_keys(self, key):\n",
    "        # N, n_heads, L, d_k\n",
    "        self.proj_key = self.make_chunks(self.linear_key(key))\n",
    "        self.proj_value = self.make_chunks(self.linear_value(key))\n",
    "\n",
    "    def score_function(self, query):\n",
    "        # Scaled do product\n",
    "        proj_query = self.make_chunks(self.linear_query(query))\n",
    "        # N, n_heads, L, d_k x N, n_heads, d_k, L ->\n",
    "        # N, n_heads, L, L\n",
    "        dot_products = torch.matmul(proj_query, self.proj_key.transpose(-2,-1))\n",
    "        scores = dot_products / np.sqrt(self.d_k)\n",
    "        return scores\n",
    "\n",
    "    def attn(self, query, mask=None):\n",
    "        # Query is batch-first: N, L, D\n",
    "        # Score function will generate scores for each head\n",
    "        scores = self.score_function(query) # N, n_heads, L, L\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        alphas = F.softmax(scores, dim=-1) # N, n_heads, L, L\n",
    "\n",
    "        alphas = self.dropout(alphas)\n",
    "        self.alpha = alphas.detach()\n",
    "\n",
    "        # N, n_heads, L, L x N, n_heads, L, d_k =>\n",
    "        # N, n_heads, L, d_k\n",
    "        context = torch.matmul(alphas, self.proj_value)\n",
    "        return context\n",
    "\n",
    "    def output_function(self, contexts):\n",
    "        # N, L, D\n",
    "        out = self.linear_out(contexts) # N, L, D\n",
    "        return out\n",
    "\n",
    "    def forward(self, query, mask=None):\n",
    "        if mask is not None:\n",
    "            # N, 1, L, L - every head uses the same maks\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # N, n_heads, L, d_k\n",
    "        context = self.attn(query, mask=mask)\n",
    "        # N, L, n_heads, d_k\n",
    "        context = context.transpose(1,2).contiguous()\n",
    "        # N, L, n_heads * d_k = N, L, d_model\n",
    "        context = context.view(query.size(0), -1, self.d_model)\n",
    "        # N, L, d_model\n",
    "        out = self.output_function(context)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8302e020-23cd-4b50-b4cf-fc6e0388379e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_points = torch.randn(16,2, 4)  # N, L, F\n",
    "mha = MultiHeadedAttention(n_heads=2, d_model=4, dropout=0.0)\n",
    "mha.init_keys(dummy_points)\n",
    "out = mha(dummy_points) # N, L, D\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed1a59-e9b4-4ec7-ad9d-3cc8eb8d58d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "062daefd-9491-413c-a7ca-99adbbc1070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, ff_units, droput=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.ff_units = ff_units\n",
    "        self.self_attn_heads = MultiHeadedAttention(n_heads, d_model, droput)\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, ff_units), \n",
    "                                nn.ReLU(),\n",
    "                                nn.Droput(droput),\n",
    "                                nn.Linear(ff_units, d_model),)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, mask=None):\n",
    "        # Sublayer #0\n",
    "        # Norm\n",
    "        norm_query = self.norm1(query)\n",
    "        # Multi-headed Attention\n",
    "        self.self_attn_heads.init_keys(norm_query)\n",
    "        states = self.self_attn_heads(norm_query, mask)\n",
    "        # Add\n",
    "        att = query + self.drop1(states)\n",
    "\n",
    "        # Sublayer #1\n",
    "        # Norm\n",
    "        norm_att = self.norm2(att)\n",
    "        # Feed Forward\n",
    "        out = self.ffn(norm_att)\n",
    "        # Add\n",
    "        out = att + self.drop2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "024fe42b-2534-48ab-8d41-48441177afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderTransf(nn.Module):\n",
    "    def __init__(self, encoder_layer, n_layer=1, max_len=100):\n",
    "        super().__init__()\n",
    "        self.d_model = encoder_layer.d_model\n",
    "        self.pe = PositionalEncoding(max_len, self.d_model)\n",
    "        self.norm = nn.LayerNorm(self.d_model)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, query, mask=None):\n",
    "        # Positional Encoding\n",
    "        x = self.pe(query)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        # Norm\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e9e925b-e33c-4577-a59e-53c3f32c2a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, ff_units, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.ff_units = ff_units\n",
    "        self.self_attn_heads = MultiHeadedAttention(n_heads, d_model, \n",
    "                                                    dropout=dropout)\n",
    "        self.cross_attn_heads = MultiHeadedAttention(n_heads, d_model,\n",
    "                                                     dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_units, d_model),\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "                \n",
    "    def init_keys(self, states):\n",
    "        self.cross_attn_heads.init_keys(states)\n",
    "         \n",
    "    def forward(self, query, source_mask=None, target_mask=None):\n",
    "        # Sublayer #0\n",
    "        # Norm\n",
    "        norm_query = self.norm1(query)\n",
    "        # Masked Multi-head Attention\n",
    "        self.self_attn_heads.init_keys(norm_query)\n",
    "        states = self.self_attn_heads(norm_query, target_mask)\n",
    "        # Add\n",
    "        att1 = query + self.drop1(states)\n",
    "        \n",
    "        # Sublayer #1\n",
    "        # Norm\n",
    "        norm_att1 = self.norm2(att1)\n",
    "        # Multi-head Attention\n",
    "        encoder_states = self.cross_attn_heads(norm_att1, source_mask)\n",
    "        # Add\n",
    "        att2 = att1 + self.drop2(encoder_states)\n",
    "        \n",
    "        # Sublayer #2\n",
    "        # Norm\n",
    "        norm_att2 = self.norm3(att2)\n",
    "        # Feed Forward\n",
    "        out = self.ffn(norm_att2)\n",
    "        # Add\n",
    "        out = att2 + self.drop3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae50cc8b-9c3f-4fa2-877d-9bcea6440cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransf(nn.Module):\n",
    "    def __init__(self, decoder_layer, n_layers=1, max_len=100):\n",
    "        super(DecoderTransf, self).__init__()\n",
    "        self.d_model = decoder_layer.d_model\n",
    "        self.pe = PositionalEncoding(max_len, self.d_model)\n",
    "        self.norm = nn.LayerNorm(self.d_model)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "    def init_keys(self, states):\n",
    "        for layer in self.layers:\n",
    "            layer.init_keys(states)\n",
    "    \n",
    "    def forward(self, query, source_mask=None, target_mask=None):\n",
    "        # Positional Encoding\n",
    "        x = self.pe(query)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, source_mask, target_mask)\n",
    "        # Norm\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20ef049d-6d12-43f5-b854-ecbfa66080c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.8049,  1.9899, -1.7325,  2.1359],\n",
       "         [ 1.7854,  0.8155,  0.1116, -1.7420]],\n",
       "\n",
       "        [[-2.4273,  1.3559,  2.8615,  2.0084],\n",
       "         [-1.0353, -1.2766, -2.2082, -0.6952]],\n",
       "\n",
       "        [[-0.8044,  1.9707,  3.3704,  2.0587],\n",
       "         [ 4.2256,  6.9575,  1.4770,  2.0762]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4\n",
    "seq_len = 2\n",
    "n_points = 3\n",
    "\n",
    "torch.manual_seed(34)\n",
    "data = torch.randn(n_points, seq_len, d_model)\n",
    "pe = PositionalEncoding(seq_len, d_model)\n",
    "inputs = pe(data)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e480666b-7b2d-47f9-ba53-3391ede48c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3529],\n",
       "         [ 0.2426]],\n",
       "\n",
       "        [[ 0.9496],\n",
       "         [-1.3038]],\n",
       "\n",
       "        [[ 1.6489],\n",
       "         [ 3.6841]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_mean = inputs.mean(axis=2).unsqueeze(2)\n",
    "inputs_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a311b77b-c86e-426f-b0ef-0534fe41471f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6.3756],\n",
       "         [1.6661]],\n",
       "\n",
       "        [[4.0862],\n",
       "         [0.3153]],\n",
       "\n",
       "        [[2.3135],\n",
       "         [4.6163]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_var = inputs.var(axis=2, unbiased=False).unsqueeze(2)\n",
    "inputs_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "348508d3-5a7b-4eb6-a661-fd961c2fb93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3671,  0.9279, -0.5464,  0.9857],\n",
       "         [ 1.1953,  0.4438, -0.1015, -1.5376]],\n",
       "\n",
       "        [[-1.6706,  0.2010,  0.9458,  0.5238],\n",
       "         [ 0.4782,  0.0485, -1.6106,  1.0839]],\n",
       "\n",
       "        [[-1.6129,  0.2116,  1.1318,  0.2695],\n",
       "         [ 0.2520,  1.5236, -1.0272, -0.7484]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inputs - inputs_mean)/torch.sqrt(inputs_var+1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72d2189a-ab57-431e-8489-3f25cf60c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_patches(x, kernel_size, stride=1):\n",
    "    # Extract patches\n",
    "    patches = x.unfold(2, kernel_size, stride)\n",
    "    patches = patches.unfold(3, kernel_size, stride)\n",
    "    patches = patches.permute(0, 2,3,1,4,5).contiguous()\n",
    "\n",
    "    return patches.view(x.shape[0], patches.shape[1], patches.shape[2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "961b13bd-3bac-41d3-b35b-6d89807c89e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = generate_dataset(img_size=12, n_images=1000, binary=False, seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20efe869-a735-4501-b8bf-fc9915a933e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.as_tensor(images[2]).unsqueeze(0).float()/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3f7cdc5-2ec8-4a9d-ba5a-8539e3333dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 16])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size=4\n",
    "paches = extract_images_patches(img, kernel_size, stride=kernel_size)\n",
    "paches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2aa2c082-ca3f-4acf-b8a1-6cfbc61d8142",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.patches' has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplots\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchapter9\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplots\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchapter10\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 5\u001b[0m fig \u001b[38;5;241m=\u001b[39m plot_patches(patches, kernel_size\u001b[38;5;241m=\u001b[39mkernel_size)\n",
      "File \u001b[1;32mD:\\notebooks\\notebooks\\plots\\chapter10.py:39\u001b[0m, in \u001b[0;36mplot_patches\u001b[1;34m(patches, kernel_size)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_patches\u001b[39m(patches, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m---> 39\u001b[0m     n, p1, p2, v \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     40\u001b[0m     fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(p1, p2, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(p1):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.patches' has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27607e69-5802-414d-839f-0ecc46b63187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
